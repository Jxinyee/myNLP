{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  20420\n"
     ]
    }
   ],
   "source": [
    "#这是对Fasttext的一个简单尝试 包括tf版本\\pytorch版本\n",
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_len = 0\n",
    "    with open(data_path, 'r',encoding='utf-8') as f:\n",
    "        for line in f.read().splitlines():\n",
    "            line_list = line.split('\\t')\n",
    "            one_data = line_list[1].split(' ')\n",
    "            tmp_len = len(one_data)\n",
    "            if tmp_len > max_sentence_len:\n",
    "                max_sentence_len = tmp_len\n",
    "            data.append(one_data)\n",
    "            labels.append(int(line_list[2]))\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels\n",
    "\n",
    "data_path = './data/AI_law/sample_seg_train.txt'\n",
    "data, labels = load_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 43727\n('马福才', 4)\n"
     ]
    }
   ],
   "source": [
    "def bulid_vocabulary(data,min_count =3):\n",
    "    count = [('<UNK>', -1), ('<PAD>', -1)]\n",
    "    words =[]\n",
    "    [words.extend(line) for line in data]\n",
    "    counter = collections.Counter(words)\n",
    "    counter_list = counter.most_common()\n",
    "    for word,c in counter_list:\n",
    "        #记录最少出现三次的词\n",
    "        if c>min_count:\n",
    "            count.append((word,c))\n",
    "        # 同理也可以限制最多出现的词的数目\n",
    "    dict_word2index = {word:c for word,c in enumerate(count)}\n",
    "    dict_index2word ={c:word for word,c in enumerate(count)}\n",
    "    print(\"vocab size:\", len(count))\n",
    "    print(count[-1])\n",
    "    return count, dict_word2index, dict_index2word\n",
    "count, dict_word2index, dict_index2word =bulid_vocabulary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data[:5]\n",
    "def build_dataset(data, labels, dict_word2index, max_sentence_len=1000, label_size=8):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        new_labels.append(labels[i]-1) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line[:max_sentence_len])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int64), np.array(new_labels, dtype=np.int64)\n",
    "\n",
    "train_data, train_labels = build_dataset(data, labels, dict_word2index, max_sentence_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1000)\n(7000,)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, radio=0.7):\n",
    "    \"\"\"\n",
    "    将训练集分给为训练集和检验集\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * radio)\n",
    "    new_data1 = data[ : split_index]\n",
    "    new_data2 = data[split_index : ]\n",
    "    return new_data1, new_data2\n",
    "train_X, valid_X = split_data(train_data)\n",
    "train_y, valid_y = split_data(train_labels)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class LawData(data.Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        super(LawData,self).__init__()\n",
    "        self.len = X.shape[0]\n",
    "        self.x_data = X\n",
    "        self.y_data = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "dataset = LawData(train_X, train_y)\n",
    "train_loader = data.DataLoader(dataset=dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,\n",
    "                               num_workers=num_workers)\n",
    "dataset = LawData(valid_X, valid_y)\n",
    "valid_loader = data.DataLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextpy(\n  (embedding): Embedding(100000, 128)\n  (linear): Linear(in_features=128, out_features=8, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import  tensorflow as tf\n",
    "#用tensorflow和pytorch俩种方法试着实现\n",
    "class FastTextpy(nn.Module):\n",
    "    #这里的config我们可以定一个config结构 去传入\n",
    "    def __init__(self,vocab_size,embedding_dim,num_classes):\n",
    "        super(FastTextpy,self).__init__()\n",
    "        self.embedding =nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim,num_classes)\n",
    "    def forward(self, input):\n",
    "        self.embeded = self.embedding(input)\n",
    "        text_embed= torch.mean(self.embeded,dim=1)\n",
    "        #print(text_embed.size())\n",
    "        text_embed = text_embed.view(-1, text_embed.size(2))\n",
    "        logits = self.linear(text_embed)\n",
    "        return logits\n",
    "\n",
    "vocab_size = 100000\n",
    "embedding_size = 128\n",
    "num_class = 8\n",
    "fast_text = FastTextpy(vocab_size=vocab_size, embedding_dim=embedding_size,\n",
    "                    num_classes=num_class)\n",
    "print(fast_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=fast_text.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个损失函数是替代loss_fun = nn.CrossEntropyLoss()\n",
    "def log_softmax(input):\n",
    "    return torch.log_softmax(input,dim =1)\n",
    "def loss_fun(inputs,label):\n",
    "    loss =0\n",
    "    for i in range(len(label)):\n",
    "        loss+=log_softmax(inputs)[i][label[i]]\n",
    "    loss=loss/len(label)\n",
    "    return -loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-014cf5fb9a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-c9c3c13ee2d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtext_embed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtext_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "epoch_num =3\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        texts,labels =data\n",
    "        inputs,labels =Variable(texts),Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fast_text(inputs)\n",
    "        loss = loss_fun(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立评分标准f1-score\n",
    "from collections import Counter\n",
    "#这是单一一个评分\n",
    "def f1(predictions,true_labels,label):\n",
    "    true_pos ,false_neg = 0,0\n",
    "    false_pos =0\n",
    "    for i in range(len(true_labels)):\n",
    "        if predictions[i] ==label:\n",
    "            if true_labels[i] ==label:\n",
    "                true_pos+=1\n",
    "            else:\n",
    "                false_pos+=1\n",
    "        else:\n",
    "            if true_labels[i] ==label:\n",
    "                false_neg+=1\n",
    "    if true_pos ==0:\n",
    "        precision,recall =0,0\n",
    "    else:\n",
    "        precision =true_pos/float(true_pos+false_pos)\n",
    "        recall = true_pos/float(true_pos+false_pos)\n",
    "    if precision ==0:\n",
    "        f1 =0\n",
    "    else:\n",
    "        f1 =(2*precision*recall)/(precision+recall)\n",
    "    return f1\n",
    "def micro_avg_f1(predict_label, true_labels, num_class):\n",
    "    true_labels = np.array(true_labels)\n",
    "    count = Counter(true_labels)\n",
    "    print(count)\n",
    "    score = 0\n",
    "    for i in range(num_class):\n",
    "        score+=(count[i]*f1(predict_label,true_labels,i))\n",
    "    score =score/float(len(true_labels))\n",
    "    return score\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(5), tensor(6), tensor(1), tensor(6), tensor(1), tensor(4), tensor(1), tensor(6), tensor(5), tensor(5)]\n[tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6), tensor(6)]\nCounter({6: 705, 1: 520, 5: 499, 0: 433, 2: 345, 4: 336, 3: 147, 7: 15})\nMicro-Averaged F1: 0.055224999999999996\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "num_class=8\n",
    "for data in valid_loader:\n",
    "    texts, labels = data\n",
    "    outputs = fast_text(Variable(texts))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    true_labels.extend(labels)\n",
    "    #predicted = [i for i in predicted]\n",
    "    predicted_labels.extend(predicted)\n",
    "\n",
    "print(true_labels[:10])\n",
    "print(predicted_labels[:20])\n",
    "print(\"Micro-Averaged F1:\",micro_avg_f1(predicted_labels, true_labels, num_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 433, 1: 520, 2: 345, 3: 147, 4: 336, 5: 499, 6: 705, 7: 15})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.array(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(494.3237, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-aa4049215318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
