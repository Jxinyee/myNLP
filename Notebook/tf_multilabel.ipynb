{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path ='./data/AI_law/sample_seg_train.txt'\n",
    "def load_data(data_path):\n",
    "    data =[]\n",
    "    ids =[]\n",
    "    max_len =0\n",
    "    law_labels =[]\n",
    "    money_label =[]\n",
    "    with open(data_path,'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lines = line.split('\\t')\n",
    "            ids.append(lines[0])\n",
    "\n",
    "            sentence = lines[1].split(' ')\n",
    "            tmp_len = len(sentence)\n",
    "            if(tmp_len>max_len):\n",
    "                max_len = tmp_len\n",
    "            multi_label =[int(i)-1 for i in lines[3].split(',')]\n",
    "            data.append(sentence)\n",
    "            law_labels.append(multi_label)\n",
    "            money_label.append(lines[2])\n",
    "    return ids , data, money_label,law_labels\n",
    "ids,data_x,_,data_y = load_data(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-66-56502f995feb>, line 7)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-56502f995feb>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    return datax[indices[:train_num],datax[indices[train_num:]],labeldata[indices[:train_num]],labeldata[indices[train_num:]]\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def split_data(datax,labeldata):\n",
    "    \n",
    "    indices = np.arange(len(datax))\n",
    "    np.random.shuffle(indices)\n",
    "    train_num = int(len(datax)*0.7)\n",
    "    print(train_num)\n",
    "    return datax[indices[:train_num],datax[indices[train_num:]],labeldata[indices[:train_num]],labeldata[indices[train_num:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n"
     ]
    }
   ],
   "source": [
    "pklf = open('./word2idx.pkl','rb')\n",
    "dict_word2index = pickle.load(pklf)\n",
    "def bulid_dataset(ids,data,labels,dict_word2index,text_maxlen,num_classes):\n",
    "    datasets=[]\n",
    "    new_ids =[]\n",
    "    new_labels =[]\n",
    "    for i in range(len(labels)):\n",
    "        new_ids.append('train_'+ids[i])\n",
    "        multi_label = np.zeros(num_classes)\n",
    "        for li in labels[i]:\n",
    "            if(li>num_classes):\n",
    "                print(li)\n",
    "                labels[i].remove(li)\n",
    "        multi_label[labels[i]] =1\n",
    "        new_labels.append(multi_label)\n",
    "        new_line =[]\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                new_line.append(dict_word2index[word])\n",
    "            else:\n",
    "                new_line.append(0)#UNK\n",
    "        pad_num = text_maxlen-len(new_line)\n",
    "        while pad_num>0:\n",
    "            new_line.append(1)#PAD\n",
    "            pad_num-=1\n",
    "        datasets.append(new_line[:text_maxlen])\n",
    "    return new_ids,np.array(datasets,dtype=np.int64),np.array(new_labels, dtype=np.int64)\n",
    "\n",
    "ids,data,multilabel = bulid_dataset(ids,data_x,data_y,dict_word2index,text_maxlen=2000,num_classes=452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x,valid_data_x,train_data_y,valid_data_y=split_data(data,multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 452)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Fasttexttf:\n",
    "    def __init__(self,config):\n",
    "        #设置超参数\n",
    "        #self.cofig = config\n",
    "        self.label_size = config.label_size #e.g.1999\n",
    "        self.batch_size = config.batch_size\n",
    "        self.decay_steps, self.decay_rate = config.decay_steps, config.decay_rate\n",
    "        self.sentence_len=config.sentence_len\n",
    "        self.vocab_size=config.vocab_size\n",
    "        self.embed_size=config.embed_size\n",
    "        self.is_training=config.is_training\n",
    "        self.learning_rate=config.learning_rate\n",
    "        self.max_label_per_example=config.max_label_per_example\n",
    "        self.initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "        #add 占位符\n",
    "        self.sentence = tf.placeholder(tf.int32,[None,self.sentence_len],name=\"sentence\")\n",
    "        self.label = tf.placeholder(tf.int32,[None,self.max_label_per_example],name=\"Labels\")\n",
    "        self.label_l199 =tf.placeholder(tf.float32,[None,self.label_size])\n",
    "\n",
    "        # 设置变量\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name=\"Epoch_Step\")\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        \n",
    "        self.init_weight()\n",
    "        self.logits = self.inference()\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "\n",
    "\n",
    "    def init_weight(self):\n",
    "        self.Embedding = tf.get_variable(\"Embedding\", [self.vocab_size, self.embed_size],initializer=self.initializer)\n",
    "        self.W = tf.get_variable(\"W\", [self.embed_size, self.label_size],initializer=self.initializer)\n",
    "        self.b = tf.get_variable(\"b\", [self.label_size])\n",
    "    def inference(self):\n",
    "        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding,self.sentence)  # [None,self.sentence_len,self.embed_size]\n",
    "\n",
    "        # 2.average vectors, to get representation of the sentence\n",
    "        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1)  # [None,self.embed_size]\n",
    "\n",
    "        # 3.linear classifier layer\n",
    "        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #[None, self.label_size]==tf.matmul([None,self.embed_size],[self.embed_size,self.label_size])\n",
    "        return logits\n",
    "        \n",
    "    def loss(self,l2_lambda =0.001):\n",
    "        labels_multi_hot = self.label_l199  # [batch_size,label_size]\n",
    "        # sigmoid_cross_entropy_with_logits:Computes sigmoid cross entropy given `logits`.Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive.  For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_multi_hot,\n",
    "                                                       logits=self.logits)  # labels:[batch_size,label_size];logits:[batch, label_size]\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))  # reduce_sum\n",
    "        print(\"loss:\", loss)\n",
    "\n",
    "        # add regularization result in not converge\n",
    "        self.l2_losses = tf.add_n(\n",
    "            [tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "        print(\"l2_losses:\", self.l2_losses)\n",
    "        loss = loss + self.l2_losses\n",
    "        return loss\n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate,global_step=self.global_step,decay_rate=self.decay_rate,staircase=True,decay_steps=self.decay_steps)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: Tensor(\"Mean_1:0\", shape=(), dtype=float32)\nl2_losses: Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX[start:end]: [[0 0 0 ... 1 1 1]\n [0 0 0 ... 1 1 1]\n [0 0 0 ... 1 1 1]\n ...\n [0 0 0 ... 1 1 1]\n [0 0 0 ... 1 1 1]\n [0 0 0 ... 1 1 1]]\ntrain_Y_batch: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tBatch 50\tTrain Loss:252.597\tL2 Loss:25.452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tBatch 100\tTrain Loss:162.760\tL2 Loss:12.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tBatch 150\tTrain Loss:118.391\tL2 Loss:6.211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tBatch 200\tTrain Loss:93.760\tL2 Loss:3.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to increment epoch counter....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\tBatch 50\tTrain Loss:15.595\tL2 Loss:2.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\tBatch 100\tTrain Loss:14.958\tL2 Loss:1.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\tBatch 150\tTrain Loss:14.516\tL2 Loss:1.517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\tBatch 200\tTrain Loss:14.176\tL2 Loss:1.419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to increment epoch counter....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tBatch 50\tTrain Loss:12.885\tL2 Loss:1.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tBatch 100\tTrain Loss:12.824\tL2 Loss:1.350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tBatch 150\tTrain Loss:12.785\tL2 Loss:1.342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tBatch 200\tTrain Loss:12.730\tL2 Loss:1.339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to increment epoch counter....\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "class Config:\n",
    "    label_size = 452\n",
    "    batch_size =32\n",
    "    over_sample = True\n",
    "    sentence_len = 2000\n",
    "    embed_size = 252\n",
    "    is_training = True\n",
    "    learning_rate =0.001\n",
    "    max_label_per_example = 5\n",
    "    decay_steps =200\n",
    "    decay_rate=0.9\n",
    "    ckpt_dir ='./checkpoints/'\n",
    "    num_epochs =3\n",
    "    vocab_size =0\n",
    "config1=tf.ConfigProto()\n",
    "config1.gpu_options.allow_growth=True\n",
    "config2 = Config()\n",
    "config2.vocab_size= len(dict_word2index)\n",
    "tf.reset_default_graph()\n",
    "with tf.Session(config=config1) as sess:\n",
    "    \n",
    "    #Instantiate Model\n",
    "    fast_text=Fasttexttf(config2)\n",
    "    #Initialize Save\n",
    "    saver=tf.train.Saver()\n",
    "    if os.path.exists(config2.ckpt_dir+\"checkpoint\"):\n",
    "        print(\"Restoring Variables from Checkpoint\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(config2.ckpt_dir))\n",
    "    else:\n",
    "        print('Initializing Variables')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    curr_epoch=sess.run(fast_text.epoch_step)\n",
    "    #3.feed data & training\n",
    "    number_of_training_data=len(train_data_x)\n",
    "    batch_size=config2.batch_size\n",
    "    for epoch in range(curr_epoch,config2.num_epochs):#range(start,stop,step_size)\n",
    "        loss, acc, counter = 0.0, 0.0, 0\n",
    "        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n",
    "            #train_Y_batch=process_labels(trainY[start:end],number=start)\n",
    "            curr_loss,current_l2_loss,_=sess.run([fast_text.loss_val,fast_text.l2_losses,fast_text.train_op],\n",
    "                                                 feed_dict={fast_text.sentence:train_data_x[start:end],fast_text.label_l199:train_data_y[start:end]}) #fast_text.labels_l1999:trainY1999[start:end]\n",
    "            if epoch==0 and counter==0:\n",
    "                print(\"trainX[start:end]:\",train_data_x[start:end]) #2d-array. each element slength is a 100.\n",
    "                print(\"train_Y_batch:\",train_data_y[start:end]) #a list,each element is a list.element:may be has 1,2,3,4,5 labels.\n",
    "                #print(\"trainY1999[start:end]:\",trainY1999[start:end])\n",
    "            loss,counter=loss+curr_loss,counter+1 #acc+curr_acc,\n",
    "            if counter %50==0:\n",
    "                print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tL2 Loss:%.3f\" %(epoch,counter,loss/float(counter),current_l2_loss)) #\\tTrain Accuracy:%.3f--->,acc/float(counter)           \n",
    "        print(\"going to increment epoch counter....\")\n",
    "        sess.run(fast_text.epoch_increment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
