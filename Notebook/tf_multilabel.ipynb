{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch.nn.functional\n",
    "import math\n",
    "def load_data(data_path):\n",
    "    data =[]\n",
    "    ids =[]\n",
    "    max_len =0\n",
    "    law_labels =[]\n",
    "    money_label =[]\n",
    "    with open(data_path,'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lines = line.split('\\t')\n",
    "            ids.append(lines[0])\n",
    "\n",
    "            sentence = lines[1].split(' ')\n",
    "            tmp_len = len(sentence)\n",
    "            if(tmp_len>max_len):\n",
    "                max_len = tmp_len\n",
    "            multi_label =[int(i)-1 for i in lines[3].split(',')]\n",
    "            data.append(sentence)\n",
    "            law_labels.append(multi_label)\n",
    "            money_label.append(lines[2])\n",
    "    return ids , data, money_label,law_labels\n",
    "data_path ='./data/AI_law/sample_seg_train.txt'\n",
    "ids,data,money_label,law_labels = load_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1000:8421\n1001-2000:950\n2001-3000:331\n3001-4000:125\n4001-5000:70\n5001-6000:36\n6001-7000:16\n7001-8000:11\n8001-9000:11\n9001-10000:12\n10001-11000:2\n11001-12000:1\n12001-13000:5\n14001-15000:3\n15001-16000:1\n16001-17000:1\n17001-18000:3\n20001-21000:1\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "def text_len_distrubution(data):\n",
    "    len_list = [len(text) for text in data]\n",
    "    #print(len_list[:20])\n",
    "    step =1000\n",
    "    for k, v in groupby(sorted(len_list),key= lambda x:(x-1)//step):\n",
    "        print('{}-{}'.format(k*step+1, (k+1)*step)+\":\"+str(len(list(v))))\n",
    "text_len_distrubution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 43727\n('马福才', 4)\n"
     ]
    }
   ],
   "source": [
    "def bulid_vocabulary(data,min_count =3):\n",
    "    count = [('<UNK>', -1), ('<PAD>', -1)]\n",
    "    words =[]\n",
    "    [words.extend(line) for line in data]\n",
    "    counter = Counter(words)\n",
    "    counter_list = counter.most_common()\n",
    "    for word,c in counter_list:\n",
    "        #记录最少出现三次的词\n",
    "        if c>min_count:\n",
    "            count.append((word,c))\n",
    "        # 同理也可以限制最多出现的词的数目\n",
    "    dict_word2index = {word:c for word,c in enumerate(count)}\n",
    "    dict_index2word ={c:word for word,c in enumerate(count)}\n",
    "    print(\"vocab size:\", len(count))\n",
    "    print(count[-1])\n",
    "    return count, dict_word2index, dict_index2word\n",
    "count, dict_word2index, dict_index2word =bulid_vocabulary(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bulid_dataset(ids,data,labels,dict_word2index,text_maxlen,num_classes):\n",
    "    datasets=[]\n",
    "    new_ids =[]\n",
    "    new_labels =[]\n",
    "    for i in range(len(labels)):\n",
    "        new_ids.append('train_'+ids[i])\n",
    "        multi_label = np.zeros(num_classes)\n",
    "        for li in labels[i]:\n",
    "            if(li>num_classes):\n",
    "                print(li)\n",
    "                labels[i].remove(li)\n",
    "        multi_label[labels[i]] =1\n",
    "        new_labels.append(multi_label)\n",
    "        new_line =[]\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                new_line.append(dict_word2index[word])\n",
    "            else:\n",
    "                new_line.append(0)#UNK\n",
    "        pad_num = text_maxlen-len(new_line)\n",
    "        while pad_num>0:\n",
    "            new_line.append(1)\n",
    "            pad_num-=1\n",
    "        datasets.append(new_line[:text_maxlen])\n",
    "    return new_ids,np.array(datasets,dtype=np.int64),np.array(new_labels, dtype=np.int64)\n",
    "\n",
    "ids,data,multilabel = bulid_dataset(ids,data,law_labels,dict_word2index,text_maxlen=2000,num_classes=452)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_dict(dic, fpath):\n",
    "    output = open(fpath, 'wb')\n",
    "    pickle.dump(dic, output)\n",
    "\n",
    "def load_pickle(fpath):\n",
    "    pkl_f = open(fpath, 'rb')\n",
    "    return pickle.load(pkl_f)\n",
    "save_dict(dict_word2index,'./word2idx.pkl')\n",
    "save_dict(dict_index2word,'./idx2word.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132597"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_vocab_size(data):\n",
    "    vocab = set()\n",
    "    for text in data:\n",
    "        vocab|=set(text)\n",
    "    return len(vocab)\n",
    "count_vocab_size(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MultiLawdata(Dataset):\n",
    "    def __init__(self,ids,X,y):\n",
    "        self.len = len(X)\n",
    "        self.x_data = X\n",
    "        self.ids = ids\n",
    "        self.y_data = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index],self.x_data[index],self.y_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "def split_data(data,ratio =0.7):\n",
    "    index = int(len(data)*ratio)\n",
    "    return data[:index],data[index:]\n",
    "train_ids,valid_ids = split_data(ids)\n",
    "train_data,valid_data = split_data(data)\n",
    "train_labels,valid_labls = split_data(multilabel)\n",
    "datasets =MultiLawdata(train_ids,train_data,train_labels)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(datasets,batch_size=20,shuffle=True,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FastText(nn.Module):\n",
    "    \"\"\"\n",
    "    fastText model\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(FastText, self).__init__()\n",
    "        self.is_training = True\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.num_class = config.num_class\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "                                      embedding_dim=self.embedding_size)\n",
    "        self.linear = nn.Linear(in_features=self.embedding_size, \n",
    "                                out_features=self.num_class)\n",
    "   \n",
    "    def forward(self, text):\n",
    "        embed = self.embedding(text)\n",
    "        text_embed = torch.mean(embed, dim=1)\n",
    "#       \n",
    "        logits = self.linear(text_embed)\n",
    "        return logits    \n",
    "\n",
    "    def get_optimizer(self, lr, lr2, weight_decay):\n",
    "\n",
    "        return torch.optim.Adam([\n",
    "            {'params': self.linear.parameters()},\n",
    "            {'params': self.embedding.parameters(), 'lr': lr2}\n",
    "        ], lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiConfig:\n",
    "    \n",
    "\n",
    "    batch_size = 32  # 64 or larger if has cuda\n",
    "    step = 10000 // batch_size   # 3000 // batch_size if has cuda\n",
    "    num_workers = 1\n",
    "#    vocab_size = 241684\n",
    "    vocab_size = 0\n",
    "    #vocab_size = 0\n",
    "    min_count = 5\n",
    "    max_text_len = 2000\n",
    "    embedding_size = 256\n",
    "    num_class = 452\n",
    "#    num_class = 321\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    learning_rate2 = 0.0    # 0.0 if pre train emb\n",
    "    lr_decay = 0.75\n",
    "    begin_epoch = 2\n",
    "    weight_decay = 0.0\n",
    "    dropout_rate = 0.5\n",
    "    epoch_num = 6\n",
    "    epoch_step = max(1, epoch_num // 20)\n",
    "   \n",
    "\n",
    "   \n",
    "  \n",
    "config =MultiConfig()\n",
    "config.vocab_size = len(dict_index2word)\n",
    "model = FastText(config)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.MultiLabelSdeoftMarginLoss()\n",
    "optimizer = model.get_optimizer(config.learning_rate,\n",
    "                                    config.learning_rate2,\n",
    "                                    config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43727"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_label_from_output(outputs):\n",
    "    predicted = outputs.cpu().data.numpy()\n",
    "    rows,cols = np.where(predicted>=0.2)\n",
    "    predicted_multilabel = get_result(outputs.size()[0],rows,cols)\n",
    "    predicted_maxlabel = torch.argmax(outputs,dim=1)\n",
    "    predicted_len = len(predicted_maxlabel)\n",
    "    for i in range(predicted_len):\n",
    "        if predicted_multilabel[i]  == []:\n",
    "            predicted_multilabel[i].append(predicted_maxlabel[i])\n",
    "    return predicted_multilabel\n",
    "            \n",
    "def get_result(row_size,rows,cols):\n",
    "    result =[]\n",
    "    for i in range(row_size):\n",
    "        result.append([])\n",
    "        for ri,row in enumerate(rows):\n",
    "            if row>i:\n",
    "                break\n",
    "            if row==i:\n",
    "                result[row].append(cols[ri])\n",
    "    return result \n",
    "def jaccard(predicted_label, true_label):\n",
    "    # print(\"predicted labels: \", predicted_label)\n",
    "    # print(\"true labels: \", true_label)\n",
    "    p = 0\n",
    "    N = len(predicted_label)\n",
    "    predict_set_size = 0\n",
    "    true_set_size = 0\n",
    "    for i in range(N):\n",
    "        Li = set(true_label[i])\n",
    "        Lig = set(predicted_label[i])\n",
    "        p += len(Li & Lig) / len(Li | Lig)\n",
    "        \n",
    "        true_set_size += len(Li)\n",
    "        predict_set_size += len(Lig)\n",
    "\n",
    "    print(\"predict_set_size / true_set_size: \", predict_set_size / true_set_size)\n",
    "    return p / N\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0252, device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this funtion is replace for nn.MultiLabelSdeoftMarginLoss()\n",
    "def log_sigmoid(input):\n",
    "    return torch.log(torch.sigmoid(torch.tensor(input)))\n",
    "def loss_fun1(input,target):\n",
    "    loss =-(target*log_sigmoid(input)+(1-target)*log_sigmoid(-input))\n",
    "    loss = loss.mean(dim=1)\n",
    "    return loss.sum()/len(input)\n",
    "loss_fun1(model(inputs),labels.float())\n",
    "loss_fun(model(inputs),labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001 lr2: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.25\n[1,   100] loss: 0.024 ,jaccard:0.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.19801980198019803\n[1,   200] loss: 0.024 ,jaccard:0.215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.19827586206896552\n[1,   300] loss: 0.025 ,jaccard:0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001 lr2: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.2127659574468085\n[2,   100] loss: 0.024 ,jaccard:0.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.27710843373493976\n[2,   200] loss: 0.024 ,jaccard:0.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.2716049382716049\n[2,   300] loss: 0.024 ,jaccard:0.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001 lr2: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.26666666666666666\n[3,   100] loss: 0.025 ,jaccard:0.214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.23958333333333334\n[3,   200] loss: 0.025 ,jaccard:0.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_set_size / true_set_size:  0.21649484536082475\n[3,   300] loss: 0.024 ,jaccard:0.193\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "for epoch in range(3):\n",
    "    print(\"lr:\",config.learning_rate,\"lr2:\",config.learning_rate2)\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        ids, text, label = data\n",
    "        inputs,labels = Variable(text).cuda(),Variable(label).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fun(outputs,labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data\n",
    "        if i % 100 == 99:\n",
    "           \n",
    "            predict_label = get_multi_label_from_output(outputs)\n",
    "            true_labels = labels.cpu().data.numpy()\n",
    "            rows,cols = np.where(true_labels==1)\n",
    "            true_labels =get_result(outputs.size()[0],rows,cols)\n",
    "            \n",
    "            running_jaccard =jaccard(predict_label,true_labels)\n",
    "            \n",
    "            print('[%d, %5d] loss: %.3f ,jaccard:%.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100,running_jaccard))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fun(model(inputs),labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 452])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6601, -0.4950, -0.3710, -0.5720],\n        [-0.4692, -0.4011, -0.5645, -0.3755],\n        [-0.4310, -0.3171, -0.3196, -0.4620]], dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.sigmoid(torch.tensor(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06719321, 0.44543959, 0.80040412, 0.25900788],\n       [0.512974  , 0.7063108 , 0.2764153 , 0.78582022],\n       [0.61831957, 0.98562428, 0.97660465, 0.53225032]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
